// sanger-tol job config is here:
// https://github.com/sanger-tol/genomeassembly/blob/dev/conf/base.config

profiles {
    spartan {
        // Spartan limits. There is a bigmem queue with 3 TB available
        params {
            max_cpus = 72
            max_memory = 710000.MB
            max_time = 90.d
        }

        //  Note, it's tempting to use the apptainer profile, but the nf-core
        //  (and some sanger-tol) pipelines have a conditional
        //  `workflow.containerEngine == 'singularity'` that prevents using the
        //  right URL with apptainer.
        singularity {
            enabled = true
            autoMounts = true
          
 runOptions = '-B $PWD,$TMPDIR,/data --nv -H $(mktemp -d) --pwd $PWD --containall --cleanenv --writable-tmpfs'
        }

        // Submit up to 256 concurrent jobs (Setonix work partition max)
        executor {
            queueSize = 128
        }

        // Define process resource limits
        process {
            resourceLimits = [
                memory: 710000.MB,
                cpus: 72,
            ]
            executor = 'slurm'
            module = 'Apptainer/1.3.3'
            cache = 'lenient'
            stageInMode = 'symlink'
            queue = { task.memory > 710000.MB ? 'bigmem' : null }
        }
    }

    pawsey {
        // workDir = "/scratch/pawsey1132/atims/genome_launcher_testing/work/"

        params {
            max_cpus = 256
            max_memory = 1020.GB
            max_time = 4.d
        }

        //  Note, it's tempting to use the apptainer profile, but the nf-core
        //  (and some sanger-tol) pipelines have a conditional
        //  `workflow.containerEngine == 'singularity'` that prevents using the
        //  right URL with apptainer.
        singularity {
            enabled = true
            autoMounts = true
            runOptions = '-B $PWD,$TMPDIR,/scratch -H $(mktemp -d) --pwd $PWD --containall --cleanenv --writable-tmpfs'
            pullTimeout = 12.h
        }

        // Submit up to 256 concurrent jobs (Setonix work partition max)
        executor {
            queueSize = 128
        }

        // Define process resource limits
        process {
            resourceLimits = [
                memory: 1020.GB,
                cpus: 256,
            ]
            executor = 'slurm'
            module = 'singularity/4.1.0-nohost'
            cache = 'lenient'
            stageInMode = 'symlink'
            queue = { task.memory > 235520.MB ? 'highmem' : (task.time > 1.d ? 'long' : null) }
            // Try to avoid the long queue by redefining the time for jobs that
            // request more than 1.d on the first attempt. Subsequent attempts
            // won't be modified.

            // Pawsey is giving me error 125 when the OOM killer is active. Try
            // to override the default spec (which is here:
            // https://github.com/sanger-tol/genomeassembly/blob/31b508a3bd8998a27f6d06d5dc41bea4707b4a03/conf/base.config#L18)
            errorStrategy = { task.exitStatus in ((130..145) + 104 + 125) ? 'retry' : 'finish' }

            // depot.galaxy container download keeps timing out cos it's so slow. Try this instead
            withName: BUSCO_BUSCO {
                container = 'quay.io/biocontainers/busco:5.7.1--pyhdfd78af_0'
            }

            // the more recent containers (0.0.9 and 0.0.8) keep throwing errors while trying to build them
            // have reverted to the most recent one I can find that runs
            withName: PRETEXTMAP_STANDRD {
                container = 'quay.io/sanger-tol/pretext:0.0.3-yy5-c1'
                time = { 12.h * task.attempt }
            }

            withName: PRETEXTMAP_HIGHRES {
                container = 'quay.io/sanger-tol/pretext:0.0.3-yy5-c1'
                time = { 12.h * task.attempt }
            }

            withName: PRETEXT_GRAPH {
                container = 'quay.io/sanger-tol/pretext:0.0.3-yy5-c1'
            }

            // have swapped from the galaxy container to the quay one because galaxy one was giving a corrupted error
            // need to investigate if that's a problem on my end (probably) or not
            withName: GAWK_UPPER_SEQUENCE {
                container = 'quay.io/biocontainers/gawk:5.3.0'
            }

            withName: GAWK_GAP_LENGTH {
                container = 'quay.io/biocontainers/gawk:5.3.0'
            }

            withName: GAWK_CLEAN_TELOMERE {
                container = 'quay.io/biocontainers/gawk:5.3.0'
            }

            withName: GAWK_MAP_TELO {
                container = 'quay.io/biocontainers/gawk:5.3.0'
            }

            withName: GAWK_REPLACE_DOTS {
                container = 'quay.io/biocontainers/gawk:5.3.0'
            }

            withName: GAWK_RENAME_IDS {
                container = 'quay.io/biocontainers/gawk:5.3.0'
            }

            withName: GAWK_REFORMAT_INTERSECT {
                container = 'quay.io/biocontainers/gawk:5.3.0'
            }

            withName: GAWK_EXTRACT_BUSCOGENE {
                container = 'quay.io/biocontainers/gawk:5.3.0'
            }

            // shorten process time to avoid getting stuck in Pawsey's long queue
            withName: MINIMAP2_ALIGN {
                time = '12h'
            }

            // shorten process time to avoid getting stuck in Pawsey's long queue
            withName:SAMTOOLS_MERGE {
                time    = { 12.h  * task.attempt }
                cpus = 32
            }

            // shorten process time to avoid getting stuck in Pawsey's long queue
            withName: BAMTOBED_SORT {
                time    = { 12.h * task.attempt }
            }

            // increase memory and cpus to avoid OOM error
            withName: GET_PAIRED_CONTACT_BED {
                time = 12.h
                memory = 72.GB
                cpus = 12
            }

            // was having errors with COOLER_CLOAD, but after more inspection
            // I think they come from the empty pre.bed file that happened when
            // GET_PAIRED_CONTACT_BED finished with an OOM error
            withName: COOLER_CLOAD {
                container = 'quay.io/biocontainers/cooler:0.9.3--pyhdfd78af_0'
            }

        }

        aws {
            client {
                endpoint = 'https://projects.pawsey.org.au'
                s3PathStyleAccess = true
                maxConnections = 4
                maxErrorRetry = 20
                uploadMaxAttempts = 20
            }
        }
    }
}
